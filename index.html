<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>New Fire and Smoke Detection Benchmark and Method</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
	   <p>
           <strong style="color: red;">Note</strong>: Our project consists of two sub-projects: one is <strong style="color: green;"><sup>1</sup>Benchmarking Multi-Scene Fire and Smoke Detection</strong> and the other is <strong style="color: pink;"><sup>2</sup>Fire and Smoke Detection with Burning Intensity Representation</strong>.  Our Datasets release time will be Nov. 11. And our Code release time will be after Nov. 30. 
            </p>		  
	    <h1 class="title is-1 publication-title">
            <img src="static/images/logo_FSD.png" style="width: 100%; height: 100%">
	    <br>
            <sup>1</sup>Benchmarking Multi-Scene Fire and Smoke Detection
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <a href="https://xiaoyihan6.github.io/" target="_blank">Xiaoyi Han</a><sup>1</sup>, </span>
              <span class="author-block">
              <a href="https://tpcd.github.io/" target="_blank">Nan Pu</a><sup>2</sup>, </span>
              <span class="author-block">
              <a href="https://person.zju.edu.cn/fengzunlei" target="_blank">Zunlei Feng</a><sup>1</sup>, </span>
              <span class="author-block">
              <a href="https://person.zju.edu.cn/beiyj" target="_blank">Yijun Bei</a><sup>1</sup>, </span>
              <span class="author-block">
              <a href="https://person.zju.edu.cn/zhangqf" target="_blank">Qifei Zhang</a><sup>1</sup>, </span>
              <span class="author-block">
              <a href="https://faculty.hfut.edu.cn/ChengLechao/zh_CN/index.htm" target="_blank">Lechao Cheng</a><sup>3,✉</sup>, </span>
              <span class="author-block">
              <a href="https://csaic.szcu.edu.cn/2023/0721/c3057a54298/page.htm" target="_blank">Liang Xue</a><sup>4</sup>, </span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <sup>1</sup><span class="author-block">School of Software Technology, Zhejiang University, </span><sup>2</sup><span class="author-block">University of Trento, </span>
                    <br>
                    <sup>3</sup><span class="author-block">Hefei University of Technology, </span><sup>4</sup><span class="author-block">Suzhou City University</span>
                    <span class="eql-cntrb"><small><br><sup>✉</sup>Corresponding Author</small></span>
                    <br>
                    <span class="author-block">Accepted to PRCV 2025</span>
                  </div>
                    
                   <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- Arxiv link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2410.16631" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fa fa-download"></i>
                        </span>
                        <span>arxiv</span>
                      </a>
                    </span>

                    <!-- Previous Dataset link -->
                    <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Previous FSD Datasets</span>
                    </a>
                  </span>
                  
                  <!-- Our MS-FSDB link -->
                    <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>MS-FSDB</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/XiaoyiHan6/MS-FSDB" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Springer Link -->
                <span class="link-block">
                  <a href="https://link.springer.com/chapter/10.1007/978-981-97-8795-1_14" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Springer</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The current irregularities in existing public Fire and Smoke Detection (FSD) datasets have become a bottleneck in the advancement of FSD technology. Upon in-depth analysis, we identify the core issue as the lack of standardized dataset construction, uniform evaluation systems, and clear performance benchmarks. To address this issue and drive innovation in FSD technology, we systematically gather diverse resources from public sources to create a more comprehensive and refined FSD benchmark. Additionally, recognizing the inadequate coverage of existing dataset scenes, we strategically expand scenes, relabel, and standardize existing public FSD datasets to ensure accuracy and consistency. We aim to establish a standardized, realistic, unified, and efficient FSD research platform that mirrors real-life scenes closely. Through our efforts, we aim to provide robust support for the breakthrough and development of FSD technology.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
       <h2 class="subtitle has-text-centered">
        <img src="static/images/benchmark_FSD.png" alt="MS-FSDB"/>
       </h2>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Benchmark Overview -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><font color="#026900">Our MS-FSDB and other FSD Datasets Overview</font></h2>
        <div class="content has-text-justified">
          <p>
          At first, we propose a new <font color="#f44a5e">Multi-Scene Fire and Smoke Detection Benchmark (MS-FSDB)</font> <font color="#f17338">comprising 12,586 images</font>, <font color="#106ddf">depicting 2,731 scenes as illustrated in the aforementioned images</font>. Most images within our benchmark <font color="#026900">possess dimensions exceeding 600 pixels in either length or width</font>.  Unlike previous public Fire and Smoke Detection (FSD) datasets, <font color="#5b6900">our benchmark not only includes flame detection but also smoke detection tasks</font>. Additionally, it captures complex scenes featuring occlusion, multiple targets, and various viewpoints.
            <br>
            <br>
            To access MS-FSDB, please click <a href="" target="_blank"><font color="#C43779">here</font></a>.
            <br>
	    <br>
           Then, we compared our benchmark with <font color="#8CC63E">the most prevalent and easily accessible Fire and Smoke Detection (FSD) datasets</font>, subjecting these popular datasets to <font color="#11B1A9">the same secondary processing</font> as our own, including labeling and image selection, to facilitate research in the field of fire detection. These datasets include <font color="#F2685B">Fire-Smoke-Dataset<sup>1</sup></font>, <font color="009A58">Furg-Fire-Dataset<sup>2</sup></font>, <font color="#11B1A9">VisiFire<sup>3</sup></font>, <font color="#1E4F6F">FIRESENSE<sup>4</sup></font>, <font color="#F2922C">BoWFire<sup>5</sup></font>.
           <br>
           <br>
           To access these FSD datasets, please click <a href="" target="_blank"><font color="#C43779">here</font></a>.
           <br>
           <br>
          <font color="#F2685B">1.DeepQuestAI, Fire-smoke-dataset (2021). <a href="https://github.com/DeepQuestAI/Fire-Smoke-Dataset" target="_blank">URL https://github.com/DeepQuestAI/Fire-Smoke-Dataset</a>.</font>
	  <br>
	  <br>
          <font color="#009A58">2.V. H¨uttner, C. R. Steffens, S. S. da Costa Botelho, First response fire combat: Deep leaning based visible fire detection, in: 2017 Latin American Robotics Symposium (LARS) and 2017 Brazilian Symposium on Robotics (SBR), IEEE, 2017, pp. 1–6. <a href="https://doi.org/10.1109/SBR-LARS-R.2017.8215312" target="_blank">doi:10.1109/SBR-LARS-R.2017.8215312</a>.</font>
	  <br>
	  <br>
          <font color="#11B1A9">3.B. U. Toreyin, A. E. Cetin, Online detection of fire in video, in: 2007 IEEE Conference on Computer Vision and Pattern Recognition, 2007, pp. 1–5. <a href="https://doi.org/10.1109/CVPR.2007.383442" target="_blank">doi: 10.1109/CVPR.2007.383442</a>.</font>
          <br>
	  <br>
          <font color="#1E4F6F">4.K. Dimitropoulos, P. Barmpoutis, N. Grammalidis, Spatio-temporal flame modeling and dynamic texture analysis for automatic video-based fire detection, IEEE Transactions on Circuits and Systems for Video Technology 25 (2) (2015) 339–351. <a href="https://doi.org/10.1109/TCSVT.2014.2339592" target="_blank">doi:10.1109/TCSVT.2014.2339592</a>.</font>
          <br>
	  <br>
          <font color="#F2922C">5.D. Y. T. Chino, L. P. S. Avalhais, J. F. Rodrigues, A. J. M. Traina, Bowfire: Detection of fire in still images by integrating pixel color and texture analysis, in: 2015 28th SIBGRAPI Conference on Graphics, Patterns and Images, 2015, pp. 95–102. <a href="https://doi.org/10.1109/SIBGRAPI.2015.19" target="_blank">doi:10.1109/SIBGRAPI.2015.19</a>.</font>         
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Benchmark Overview -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
     <div class="container is-max-desktop">
     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Benchmark Presentation and Experimental Results</h2>
      </div>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-centered">
	<strong>Multi-Scene in FSDB</strong>: residential building, air crush, indoor clutter, electric power station, etc.
        <img src="static/images/MS_FSDB.png" alt="Multi-Scene in FSDB: residential building, air crush, indoor clutter, electric power station, etc."/>
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-centered">
There is Illustrations of several <strong>Fire and Smoke Detection (FSD)</strong> datasets statistics. <font color="#f08a5d">(a)</font> represents the statistics comparison of several FSD datasets scenes. <font color="#3490de">(b)</font> provides more detailed statistics for our benchmark. In <font color="#3490de">(b)</font>, the area of the sector corresponds to the number of scenes. Additionally, the total number of scenes equals 2731, as displayed in <font color="#f08a5d">(a)</font>.
        <img src="static/images/Several_FSD_Datasets_Statistics.png" alt="There is Illustrations of several Fire and Smoke Detection (FSD) datasets statistics. (a) represents the statistics comparison of several FSD datasets scenes. (b) provides more detailed statistics for our benchmark. In (b), the area of the sector corresponds to the number of scenes. Additionally, the total number of scenes equals 2731, as displayed in (a)."/>
        </h2> 
      </div>
      <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-centered">
Baseline models comparison across different datasets, where Fire represents the Average Precision (%) of fire, Smoke represents the Average Precision (%) of smoke, and mAP represents the mean Average Precision (%) of fire and smoke. <font color="#e84545">“s”</font> represents the input image of small size, while <font color="#14ffec">“l”</font> represents the input image of large size, and the specific content is in the subsection “Basic Experiments”. <font color="#ff2e63">F-RCNN</font> means <font color="#ff2e63">Faster RCNN</font>. Ours means our method in <sup>2</sup>Fire and Smoke Detection with Burning Intensity Representation</strong>.
	<img src="static/images/Datasets_FSD_Methods.png" alt="Baseline models comparison across different datasets, where Fire represents the Average Precision (%) of fire, Smoke represents the Average Precision (%) of smoke, and mAP represents the mean Average Precision (%) of fire and smoke.“s”represents the input image of small size, while “l” represents the input image of large size, and the specific content is in the subsection “Basic Experiments”. F-RCNN means Faster RCNN. Ours means our method in <sup>2</sup>Fire and Smoke Detection with Burning Intensity Representation</strong>."/>
        </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <h2 class="subtitle has-text-centered">
Cross experimental results in <font color="#f08a5d">(a)</font>, where results represent the mean Average Precision (%) of fire and smoke. the table on the right, we train baseline models using the proposed miniMS-FSDB and then test with previous FSD dataset. <font color="#e84545">For example</font>, a mAP of 45.8 represents the result of the SSD model trains on our miniMS-FSDB and then test using the Fire-Smoke-Dataset. The table on the left, we train baseline models using previous FSD dataset and then test with miniMS-FSDB. <font color="#e84545">For example</font>, a mAP of 45.5 represents the result of the SSD model trains on Fire-Smoke-Dataset and the test using our miniMS-FSDB. We uniformly choose small input images for all baseline models to ensure consistency and comparability in experiments. <font color="#ff2e63">F-RCNN</font> means <font color="#ff2e63">Faster RCNN</font> and Ours means our method in <sup>2</sup>Fire and Smoke Detection with Burning Intensity Representation</strong>.
      <br>
      We train baselines on different FSD datasets and then test with the new testing set in <font color="#3490de">(b)</font>, where mAP represents the mean Average Precision (%) of fire and smoke. We use the input image of small size. <font color="#ff2e63">F-RCNN</font> means <font color="#ff2e63">Faster RCNN</font>,and Ours means our method in <sup>2</sup>Fire and Smoke Detection with Burning Intensity Representation</strong>.
      <img src="static/images/MS_FSDB_Ablation_Study.png" alt="Ablation Study"/>
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
     <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@InProceedings{han2025prcv,
      author="Han, Xiaoyi and Pu, Nan and Feng, Zunlei and Bei, Yijun and Zhang, Qifei and Cheng, Lechao and Xue, Liang", 
      editor="Lin, Zhouchen and Cheng, Ming-Ming and He, Ran and Ubul, Kurban and Silamu, Wushouer and Zha, Hongbin and Zhou, Jie and Liu, Cheng-Lin", 
      title="Benchmarking Multi-Scene Fire and Smoke Detection",booktitle="Pattern Recognition and Computer Vision", 
      year="2025", publisher="Springer Nature Singapore", address="Singapore", pages="203--218", 
      abstract="The current irregularities in existing public Fire and Smoke Detection (FSD) datasets have become a bottleneck in the advancement of FSD technology. Upon in-depth analysis, we identify the core issue as the lack of standardized dataset construction, uniform evaluation systems, and clear performance benchmarks. To address this issue and drive innovation in FSD technology, we systematically gather diverse resources from public sources to create a more comprehensive and refined FSD benchmark. Additionally, recognizing the inadequate coverage of existing dataset scenes, we strategically expand scenes, relabel, and standardize existing public FSD datasets to ensure accuracy and consistency. We aim to establish a standardized, realistic, unified, and efficient FSD research platform that mirrors real-life scenes closely. Through our efforts, we aim to provide robust support for the breakthrough and development of FSD technology. The project is available at https://xiaoyihan6.github.io/FSD/.",
isbn="978-981-97-8795-1"}
</code></pre>
   </div>
</section>
<!--End BibTex citation -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <p>
    <font color="#a4a5ef">******************************************************************************************************************************************************************************************************************************************************************************************************************************************************************</font>
    </p>
    </div>
  </div>
</section>


<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
            <sup>2</sup>Fire and Smoke Detection with Burning Intensity Representation
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <a href="https://xiaoyihan6.github.io/" target="_blank">Xiaoyi Han</a><sup>1</sup>, </span>
              <span class="author-block">
              <span style="color: black;">Yanfei Wu</span><sup>2</sup>, </span>
              <span class="author-block">
              <a href="https://tpcd.github.io/" target="_blank">Nan Pu</a><sup>3</sup>, </span>
              <span class="author-block">
              <a href="https://person.zju.edu.cn/fengzunlei" target="_blank">Zunlei Feng</a><sup>1</sup>, </span>
              <span class="author-block">
              <a href="https://person.zju.edu.cn/zhangqf" target="_blank">Qifei Zhang</a><sup>1</sup>, </span>
              <span class="author-block">
              <a href="https://person.zju.edu.cn/beiyj" target="_blank">Yijun Bei</a><sup>1</sup>, </span>
              <span class="author-block">
              <a href="https://faculty.hfut.edu.cn/ChengLechao/zh_CN/index.htm" target="_blank">LeChao Cheng</a><sup>4,✉</sup>, </span>
              </div>

              <div class="is-size-5 publication-authors">

		      <sup>1</sup><span class="author-block">School of Software Technology, Zhejiang University, </span><sup>2</sup><span class="author-block">China Mobile (Suzhou) Software Technology Co., Ltd., </span>

                    <br>

                    <sup>3</sup><span class="author-block">University of Trento, </span><sup>4</sup><span class="author-block">Hefei University of Technology</span>

                    <span class="eql-cntrb"><small><br><sup>✉</sup>Corresponding Author</small></span>

                    <br>

                    <span class="author-block">Accepted to ACM MM asia 2024</span>

                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2410.16642" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>arxiv</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/XiaoyiHan6/FSDmethod" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ACM Link -->
                <span class="link-block">
                  <a href="https://dl.acm.org/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-download"></i>
                  </span>
                  <span>ACM</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           An effective Fire and Smoke Detection (FSD) and analysis system is of paramount importance due to the profound destructive potential of fire disasters. However, many existing FSD methods directly employ generic object detection techniques without accounting for the transparency of the fire and smoke, inevitably leading to imprecise localization of fire and smoke areas and consequently diminishing detection performance of fire and smoke. To address this issue, a new Attentive Transparency Detection Head (ATDH) is proposed to improve the accuracy of transparent target detection while retaining the robust feature extraction and fusion capabilities of conventional detection algorithms. In addition, Burning Intensity (BI) is introduced as a pivotal feature for fire-related downstream risk assessment in traditional FSD methodologies. Extensive experiments on multiple FSD datasets showcase the effectiveness and versatility of the proposed FSD model.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
    <div class="hero-body">
    <div class="container is-max-desktop">
     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">	    
      <h2 class="title is-3">Method and Experimental Results</h2>
      </div>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-centered">	
Our <strong>Fire and Smoke Detection Method</strong>, which is called <font color="fa7f6f"> Attentive Fire Smoke Detection Model (a-FSDM)</font>. And it consists of three main components: <font color="8ecfc9">Feature Extraction</font>, <font color="82b0d2">Feature Fusion Grouping</font>, and <font color="#ff2e63">Attentive Transparency Detection Head (ATDH) of FSD task</font>. <font color="82b0d2">Feature Fusion Grouping</font> is used to fuse semantic and spatial information from various convolutional layers to prevent information loss. <font color="#ff2e63">The ATDH</font> is responsible for classification, regression, and centerness.
        <img src="static/images/FSD_method.png" alt="Our Fire and Smoke Detection Method"/>
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-centered">
If you want to compare baseline models across different datasets, you can refer to <sup>1</sup>Benchmarking Multi-Scene Fire and Smoke Detection. 
<br>
And then, comparison between generic detection heads and <font color="#07689f">the Attention Transparency Detection Head (ATDH)</font> across the MS-FSDB is provided in <font color="#f08a5d">(a)</font>, where Fire, Smoke, and mAP are discussed in the subsection 'Setting and Details.' <font color="#e84545">“s”</font> represents the input image of small size, while <font color="#14ffec">“l”</font> represents the input image of large size.
        <img src="static/images/atdh_baselines.png" alt="ATDH Baselines"/>
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-centered">
In <font color="#8c82fc">(1)</font>, the attention mechanism algorithm is added to <font color="#15b7b9">FCOS</font> on the <font color="#b61aae">MS-FSDB</font>, where Fire, Smoke, and mAP are discussed in the subsection 'Setting and Details.' The input image of small size is used.
<br>
In <font color="#9e579d">(2)</font>, the evidence supporting the efficacy of the <font color="#62d2a2">ATDH</font>, <font color="#f73859">(a)</font> original image, <font color="#ff9a3c">(b)</font> CAM visualization results of FCOS with SENet [13], <font color="#c86b85">(c)</font> CAM visualization results of FCOS with SKNet [22]. <font color="#e61c5d">(d)</font> CAM visualization results of <font color="#15b7b9">FCOS</font> with <font color="#62d2a2">ATDH</font>.
        <img src="static/images/FSD_Ablation_Study.png" alt="Ablation Study"/>
        </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <h2 class="subtitle has-text-centered">
	      The performance of different models is shown in detail, covering seven detection models in terms of key metrics such as detection accuracy, mAP, performance (Gflops), and the number of parameters (M) for each category. Fire, Smoke, and mAP are discussed in the subsection 'Setting and Details.' We use small-sized input images from the miniMS-FSDB. In the models, <font color="#80d6ff">[17]</font> represents <font color="#80d6ff">YOLOv5</font>, <font color="#2eb872">[18]</font> stands for <font color="#2eb872">YOLOv8</font>, <font color="#ba52ed">[25]</font> denotes <font color="#ba52ed">SSD</font>, <font color="#46b7b9">[24]</font> signifies <font color="#46b7b9">RetinaNet</font>, <font color="#ff7c38">[33]</font> means <font color="#ff7c38">Faster R-CNN</font>, <font color="#1e56a0">[37]</font> refers to <font color="#1e56a0">FCOS</font>, and <font color="#f95959">'ours'</font> represents the proposed <font color="#f95959">a-FSDM model</font>. Additionally, <font color="#7098da">'s', 'x', 'n', 'l', and 'm'</font> represent different versions of the models.
      <img src="static/images/The_performance_of_different_models.png" alt="The performance of different models"/>
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{han2024mmasia,
  title={Fire and Smoke Detection with Burning Intensity Representation},
  author={Xiaoyi Han and Yanfei Wu and Nan Pu and Zunlei Feng and Qifei Zhang and Yijun Bei and Lechao Cheng},
  booktitle={The 6th ACM Multimedia Asia conference},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Widget Customization -->
<section class="hero is-small is-light">
  <div class="hero-body">
      <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=f2eada&w=a&t=tt&d=bCaAfF4KfUlj0gUrEvtZKiG9700xRaqYAX_eetSoySo&co=5b7493&ct=f0fff0'></script>
    </div>
  </section>
<!--End Widget Customization -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

  </body>
  </html>
